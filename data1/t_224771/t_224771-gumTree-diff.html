<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>Diff result</title>
<style type="text/css">
body { width: 100%; font-size: 10pt; }
h1 { font-size: 125%; }
div.content { font-family: Verdana, "DejaVu Sans Condensed", "Liberation Sans","Nimbus Sans L", Helvetica, sans-serif; margin : 1em auto; width: 100%; }
div.left { float: left; width: 48%; padding: 1em; }
div.right { float: right; width: 48%; padding: 1em; }
div.code { font-family: "Liberation Mono", "Courrier New", monospace; border:1px solid black;}
div.clear { clear: both; }
span.del { background-color : red; font-weight: normal; font-style: normal;}
span.add { background-color : lightgreen; font-weight: bold; font-style: normal;}
span.upd { background-color : orange; font-weight: bold; font-style: italic;}
span.id { background-color : white; font-weight: normal; font-style: normal;}
span.mv { background-color : yellow; font-weight: normal; font-style: normal;}
</style></head><body><div class="content"><div class="left">
<h1>left_IndexWriter_1.2.java</h1>
<div class="code">
<div class="id">
package org.apache.lucene.index;<br/>
<br/>
/* ====================================================================<br/>
 * The Apache Software License, Version 1.1<br/>
 *<br/>
 * Copyright (c) 2001 The Apache Software Foundation.  All rights<br/>
 * reserved.<br/>
 *<br/>
 * Redistribution and use in source and binary forms, with or without<br/>
 * modification, are permitted provided that the following conditions<br/>
 * are met:<br/>
 *<br/>
 * 1. Redistributions of source code must retain the above copyright<br/>
 *    notice, this list of conditions and the following disclaimer.<br/>
 *<br/>
 * 2. Redistributions in binary form must reproduce the above copyright<br/>
 *    notice, this list of conditions and the following disclaimer in<br/>
 *    the documentation and/or other materials provided with the<br/>
 *    distribution.<br/>
 *<br/>
 * 3. The end-user documentation included with the redistribution,<br/>
 *    if any, must include the following acknowledgment:<br/>
 *       "This product includes software developed by the<br/>
 *        Apache Software Foundation (http://www.apache.org/)."<br/>
 *    Alternately, this acknowledgment may appear in the software itself,<br/>
 *    if and wherever such third-party acknowledgments normally appear.<br/>
 *<br/>
 * 4. The names "Apache" and "Apache Software Foundation" and<br/>
 *    "Apache Lucene" must not be used to endorse or promote products<br/>
 *    derived from this software without prior written permission. For<br/>
 *    written permission, please contact apache@apache.org.<br/>
 *<br/>
 * 5. Products derived from this software may not be called "Apache",<br/>
 *    "Apache Lucene", nor may "Apache" appear in their name, without<br/>
 *    prior written permission of the Apache Software Foundation.<br/>
 *<br/>
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED<br/>
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES<br/>
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE<br/>
 * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR<br/>
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,<br/>
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT<br/>
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF<br/>
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND<br/>
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,<br/>
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT<br/>
 * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF<br/>
 * SUCH DAMAGE.<br/>
 * ====================================================================<br/>
 *<br/>
 * This software consists of voluntary contributions made by many<br/>
 * individuals on behalf of the Apache Software Foundation.  For more<br/>
 * information on the Apache Software Foundation, please see<br/>
 * &lt;http://www.apache.org/&gt;.<br/>
 */<br/>
<br/>
import java.io.IOException;<br/>
import java.io.File;<br/>
import java.io.PrintStream;<br/>
import java.util.Vector;<br/>
<br/>
import org.apache.lucene.store.Directory;<br/>
import org.apache.lucene.store.RAMDirectory;<br/>
import org.apache.lucene.store.FSDirectory;<br/>
import org.apache.lucene.store.Lock;<br/>
import org.apache.lucene.store.InputStream;<br/>
import org.apache.lucene.store.OutputStream;<br/>
import org.apache.lucene.document.Document;<br/>
import org.apache.lucene.analysis.Analyzer;<br/>
<br/>
/**<br/>
  An IndexWriter creates and maintains an index.<br/>
<br/>
  The third argument to the &lt;a href="#IndexWriter"&gt;&lt;b&gt;constructor&lt;/b&gt;&lt;/a&gt;<br/>
  determines whether a new index is created, or whether an existing index is<br/>
  opened for the addition of new documents.<br/>
<br/>
  In either case, documents are added with the &lt;a<br/>
  href="#addDocument"&gt;&lt;b&gt;addDocument&lt;/b&gt;&lt;/a&gt; method.  When finished adding<br/>
  documents, &lt;a href="#close"&gt;&lt;b&gt;close&lt;/b&gt;&lt;/a&gt; should be called.<br/>
<br/>
  If an index will not have more documents added for a while and optimal search<br/>
  performance is desired, then the &lt;a href="#optimize"&gt;&lt;b&gt;optimize&lt;/b&gt;&lt;/a&gt;<br/>
  method should be called before the index is closed.<br/>
  */<br/>
<br/>
public final class IndexWriter {<br/>
  private Directory directory;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // where this index resides<br/>
  private Analyzer analyzer;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // how to analyze text<br/>
<br/>
  private SegmentInfos segmentInfos = new SegmentInfos(); // the segments<br/>
  private final Directory ramDirectory = new RAMDirectory(); // for temp segs<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;path&lt;/code&gt;.  Text will<br/>
    be analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a<br/>
    new, empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index<br/>
    already there, if any. */<br/>
  public IndexWriter(String path, Analyzer a, boolean create)<br/>
       throws IOException {<br/>
    this(FSDirectory.getDirectory(path, create), a, create);<br/>
  }<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;path&lt;/code&gt;.  Text will<br/>
    be analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a<br/>
    new, empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index<br/>
    already there, if any. */<br/>
  public IndexWriter(File path, Analyzer a, boolean create)<br/>
       throws IOException {<br/>
    this(FSDirectory.getDirectory(path, create), a, create);<br/>
  }<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;d&lt;/code&gt;.  Text will be<br/>
    analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a new,<br/>
    empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index already<br/>
    there, if any. */<br/>
  public IndexWriter(Directory d, Analyzer a, final boolean create)<br/>
       throws IOException {<br/>
    directory = d;<br/>
    analyzer = a;<br/>
<br/>
    Lock writeLock = directory.makeLock("write.lock");<br/>
    if (!writeLock.obtain())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // obtain write lock<br/>
      throw new IOException("Index locked for write: " + writeLock);<br/>
<br/>
    synchronized (directory) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // in- &amp; inter-process sync<br/>
      new Lock.With(directory.makeLock("commit.lock")) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  public Object doBody() throws IOException {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    if (create)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;      segmentInfos.write(directory);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;      segmentInfos.read(directory);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    return null;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  }<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}.run();<br/>
    }<br/>
  }<br/>
<br/>
  /** Flushes all changes to an index, closes all associated files, and closes<br/>
    the directory that the index is stored in. */<br/>
  public final synchronized void close() throws IOException {<br/>
    flushRamSegments();<br/>
    ramDirectory.close();<br/>
    directory.makeLock("write.lock").release();  // release write lock<br/>
    directory.close();<br/>
  }<br/>
<br/>
  /** Returns the number of documents currently in this index. */<br/>
  public final synchronized int docCount() {<br/>
    int count = 0;<br/>
    for (int i = 0; i &lt; segmentInfos.size(); i++) {<br/>
      SegmentInfo si = segmentInfos.info(i);<br/>
      count += si.docCount;<br/>
    }<br/>
    return count;<br/>
  }<br/>
<br/>
  /** The maximum number of terms that will be indexed for a single field in a<br/>
    document.  This limits the amount of memory required for indexing, so that<br/>
    collections with very large files will not crash the indexing process by<br/>
    running out of memory.<br/>
<br/>
    &lt;p&gt;By default, no more than 10,000 terms will be indexed for a field. */<br/>
  public int maxFieldLength = 10000;<br/>
<br/>
  /** Adds a document to this index.*/<br/>
  public final void addDocument(Document doc) throws IOException {<br/>
    DocumentWriter dw =<br/>
      new DocumentWriter(ramDirectory, analyzer, maxFieldLength);<br/>
    String segmentName = newSegmentName();<br/>
    dw.addDocument(segmentName, doc);<br/>
    synchronized (this) {<br/>
      segmentInfos.addElement(new SegmentInfo(segmentName, 1, ramDirectory));<br/>
      maybeMergeSegments();<br/>
    }<br/>
  }<br/>
<br/>
  private final synchronized String newSegmentName() {<br/>
    return "_" + Integer.toString(segmentInfos.counter++, Character.MAX_RADIX);<br/>
  }<br/>
<br/>
  /** Determines how often segment indexes are merged by addDocument().  With<br/>
   * smaller values, less RAM is used while indexing, and searches on<br/>
   * unoptimized indexes are faster, but indexing speed is slower.  With larger<br/>
   * values more RAM is used while indexing and searches on unoptimized indexes<br/>
   * are slower, but indexing is faster.  Thus larger values (&gt; 10) are best<br/>
   * for batched index creation, and smaller values (&lt; 10) for indexes that are<br/>
   * interactively maintained.<br/>
   *<br/>
   * &lt;p&gt;This must never be less than 2.  The default value is 10.*/<br/>
  public int mergeFactor = 10;<br/>
<br/>
  /** Determines the largest number of documents ever merged by addDocument().<br/>
   * Small values (e.g., less than 10,000) are best for interactive indexing,<br/>
   * as this limits the length of pauses while indexing to a few seconds.<br/>
   * Larger values are best for batched indexing and speedier searches.<br/>
   *<br/>
   * &lt;p&gt;The default value is {@link Integer#MAX_VALUE}. */<br/>
  public int maxMergeDocs = Integer.MAX_VALUE;<br/>
<br/>
  /** If non-null, information about merges will be printed to this. */<br/>
  public PrintStream infoStream = null;<br/>
<br/>
  /** Merges all segments together into a single segment, optimizing an index<br/>
      for search. */<br/>
  public final synchronized void optimize() throws IOException {<br/>
    flushRamSegments();<br/>
    while (<span class="mv"><span class="upd"><span class="mv">segmentInfos.size() &gt; 1</span> ||<br/>
&nbsp;&nbsp;&nbsp;&nbsp;   (<span class="upd"><span class="mv">segmentInfos.size() == 1</span> &amp;&amp;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    SegmentReader.hasDeletions(segmentInfos.info(0))</span>)</span></span>){<br/>
      int minSegment = segmentInfos.size() - mergeFactor;<br/>
      mergeSegments(minSegment &lt; 0 ? 0 : minSegment);<br/>
    }<br/>
  }<br/>
<br/>
  /** Merges all segments from an array of indexes into this index.<br/>
   *<br/>
   * &lt;p&gt;This may be used to parallelize batch indexing.  A large document<br/>
   * collection can be broken into sub-collections.  Each sub-collection can be<br/>
   * indexed in parallel, on a different thread, process or machine.  The<br/>
   * complete index can then be created by merging sub-collection indexes<br/>
   * with this method.<br/>
   *<br/>
   * &lt;p&gt;After this completes, the index is optimized. */<br/>
  public final synchronized void addIndexes(Directory[] dirs)<br/>
      throws IOException {<br/>
    optimize();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // start with zero or 1 seg<br/>
    int minSegment = segmentInfos.size();<br/>
    int segmentsAddedSinceMerge = 0;<br/>
    for (int i = 0; i &lt; dirs.length; i++) {<br/>
      SegmentInfos sis = new SegmentInfos();&nbsp;&nbsp;&nbsp;&nbsp;  // read infos from dir<br/>
      sis.read(dirs[i]);<br/>
      for (int j = 0; j &lt; sis.size(); j++) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;segmentInfos.addElement(sis.info(j));&nbsp;&nbsp;&nbsp;&nbsp;  // add each info<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;// merge whenever mergeFactor segments have been added<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (++segmentsAddedSinceMerge == mergeFactor) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  mergeSegments(minSegment++, false);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  segmentsAddedSinceMerge = 0;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
      }<br/>
    }<br/>
    optimize();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // final cleanup<br/>
  }<br/>
<br/>
  /** Merges all RAM-resident segments. */<br/>
  private final void flushRamSegments() throws IOException {<br/>
    int minSegment = segmentInfos.size()-1;<br/>
    int docCount = 0;<br/>
    while (minSegment &gt;= 0 &amp;&amp;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;   (segmentInfos.info(minSegment)).dir == ramDirectory) {<br/>
      docCount += segmentInfos.info(minSegment).docCount;<br/>
      minSegment--;<br/>
    }<br/>
    if (minSegment &lt; 0 ||&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // add one FS segment?<br/>
&nbsp;&nbsp;&nbsp;&nbsp;(docCount + segmentInfos.info(minSegment).docCount) &gt; mergeFactor ||<br/>
&nbsp;&nbsp;&nbsp;&nbsp;!(segmentInfos.info(segmentInfos.size()-1).dir == ramDirectory))<br/>
      minSegment++;<br/>
    if (minSegment &gt;= segmentInfos.size())<br/>
      return;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // none to merge<br/>
    mergeSegments(minSegment);<br/>
  }<br/>
<br/>
  /** Incremental segment merger.  */<br/>
  private final void maybeMergeSegments() throws IOException {<br/>
    long targetMergeDocs = mergeFactor;<br/>
    while (targetMergeDocs &lt;= maxMergeDocs) {<br/>
      // find segments smaller than current target size<br/>
      int minSegment = segmentInfos.size();<br/>
      int mergeDocs = 0;<br/>
      while (--minSegment &gt;= 0) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;SegmentInfo si = segmentInfos.info(minSegment);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (si.docCount &gt;= targetMergeDocs)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  break;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;mergeDocs += si.docCount;<br/>
      }<br/>
<br/>
      if (mergeDocs &gt;= targetMergeDocs)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // found a merge to do<br/>
&nbsp;&nbsp;&nbsp;&nbsp;mergeSegments(minSegment+1);<br/>
      else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;break;<br/>
      <br/>
      targetMergeDocs *= mergeFactor;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // increase target size<br/>
    }<br/>
  }<br/>
<br/>
  /** Pops segments off of segmentInfos stack down to minSegment, merges them,<br/>
    and pushes the merged index onto the top of the segmentInfos stack. */<br/>
  private final void mergeSegments(int minSegment) throws IOException {<br/>
    mergeSegments(minSegment, true);<br/>
  }<br/>
<br/>
  /** Pops segments off of segmentInfos stack down to minSegment, merges them,<br/>
    and pushes the merged index onto the top of the segmentInfos stack. */<br/>
  private final void mergeSegments(int minSegment, boolean delete)<br/>
      throws IOException {<br/>
    String mergedName = newSegmentName();<br/>
    int mergedDocCount = 0;<br/>
    if (infoStream != null) infoStream.print("merging segments");<br/>
    SegmentMerger merger = new SegmentMerger(directory, mergedName);<br/>
    final Vector segmentsToDelete = new Vector();<br/>
    for (int i = minSegment; i &lt; segmentInfos.size(); i++) {<br/>
      SegmentInfo si = segmentInfos.info(i);<br/>
      if (infoStream != null)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;infoStream.print(" " + si.name + " (" + si.docCount + " docs)");<br/>
      SegmentReader reader = new SegmentReader(si);<br/>
      merger.add(reader);<br/>
      if (delete)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;segmentsToDelete.addElement(reader);&nbsp;&nbsp;&nbsp;&nbsp;  // queue for deletion<br/>
      mergedDocCount += si.docCount;<br/>
    }<br/>
    if (infoStream != null) {<br/>
      infoStream.println();<br/>
      infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");<br/>
    }<br/>
    merger.merge();<br/>
<br/>
    segmentInfos.setSize(minSegment);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // pop old infos &amp; add new<br/>
    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    directory));<br/>
    <br/>
    synchronized (directory) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // in- &amp; inter-process sync<br/>
      new Lock.With(directory.makeLock("commit.lock")) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  public Object doBody() throws IOException {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    segmentInfos.write(directory);&nbsp;&nbsp;&nbsp;&nbsp;  // commit before deleting<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    deleteSegments(segmentsToDelete);&nbsp;&nbsp;&nbsp;&nbsp;  // delete now-unused segments<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    return null;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  }<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}.run();<br/>
    }<br/>
  }<br/>
<br/>
  /* Some operating systems (e.g. Windows) don't permit a file to be deleted<br/>
     while it is opened for read (e.g. by another process or thread).  So we<br/>
     assume that when a delete fails it is because the file is open in another<br/>
     process, and queue the file for subsequent deletion. */<br/>
<br/>
  private final void deleteSegments(Vector segments) throws IOException {<br/>
    Vector deletable = new Vector();<br/>
<br/>
    deleteFiles(readDeleteableFiles(), deletable); // try to delete deleteable<br/>
    <br/>
    for (int i = 0; i &lt; segments.size(); i++) {<br/>
      SegmentReader reader = (SegmentReader)segments.elementAt(i);<br/>
      if (reader.directory == this.directory)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;deleteFiles(reader.files(), deletable);&nbsp;&nbsp;&nbsp;&nbsp;  // try to delete our files<br/>
      else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;deleteFiles(reader.files(), reader.directory); // delete, eg, RAM files<br/>
    }<br/>
<br/>
    writeDeleteableFiles(deletable);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // note files we can't delete<br/>
  }<br/>
<br/>
  private final void deleteFiles(Vector files, Directory directory)<br/>
       throws IOException {<br/>
    for (int i = 0; i &lt; files.size(); i++)<br/>
      directory.deleteFile((String)files.elementAt(i));<br/>
  }<br/>
<br/>
  private final void deleteFiles(Vector files, Vector deletable)<br/>
       throws IOException {<br/>
    for (int i = 0; i &lt; files.size(); i++) {<br/>
      String file = (String)files.elementAt(i);<br/>
      try {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;directory.deleteFile(file);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // try to delete each file<br/>
      } catch (IOException e) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // if delete fails<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (directory.fileExists(file)) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  if (infoStream != null)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    infoStream.println(e.getMessage() + "; Will re-try later.");<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  deletable.addElement(file);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // add to deletable<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
      }<br/>
    }<br/>
  }<br/>
<br/>
  private final Vector readDeleteableFiles() throws IOException {<br/>
    Vector result = new Vector();<br/>
    if (!directory.fileExists("deletable"))<br/>
      return result;<br/>
<br/>
    InputStream input = directory.openFile("deletable");<br/>
    try {<br/>
      for (int i = input.readInt(); i &gt; 0; i--)&nbsp;&nbsp;&nbsp;&nbsp;  // read file names<br/>
&nbsp;&nbsp;&nbsp;&nbsp;result.addElement(input.readString());<br/>
    } finally {<br/>
      input.close();<br/>
    }<br/>
    return result;<br/>
  }<br/>
<br/>
  private final void writeDeleteableFiles(Vector files) throws IOException {<br/>
    OutputStream output = directory.createFile("deleteable.new");<br/>
    try {<br/>
      output.writeInt(files.size());<br/>
      for (int i = 0; i &lt; files.size(); i++)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;output.writeString((String)files.elementAt(i));<br/>
    } finally {<br/>
      output.close();<br/>
    }<br/>
    directory.renameFile("deleteable.new", "deletable");<br/>
  }<br/>
}<br/>
</div>
</div>
</div>
<div class="right">
<h1>right_IndexWriter_1.3.java</h1>
<div class="code">
<div class="id">
package org.apache.lucene.index;<br/>
<br/>
/* ====================================================================<br/>
 * The Apache Software License, Version 1.1<br/>
 *<br/>
 * Copyright (c) 2001 The Apache Software Foundation.  All rights<br/>
 * reserved.<br/>
 *<br/>
 * Redistribution and use in source and binary forms, with or without<br/>
 * modification, are permitted provided that the following conditions<br/>
 * are met:<br/>
 *<br/>
 * 1. Redistributions of source code must retain the above copyright<br/>
 *    notice, this list of conditions and the following disclaimer.<br/>
 *<br/>
 * 2. Redistributions in binary form must reproduce the above copyright<br/>
 *    notice, this list of conditions and the following disclaimer in<br/>
 *    the documentation and/or other materials provided with the<br/>
 *    distribution.<br/>
 *<br/>
 * 3. The end-user documentation included with the redistribution,<br/>
 *    if any, must include the following acknowledgment:<br/>
 *       "This product includes software developed by the<br/>
 *        Apache Software Foundation (http://www.apache.org/)."<br/>
 *    Alternately, this acknowledgment may appear in the software itself,<br/>
 *    if and wherever such third-party acknowledgments normally appear.<br/>
 *<br/>
 * 4. The names "Apache" and "Apache Software Foundation" and<br/>
 *    "Apache Lucene" must not be used to endorse or promote products<br/>
 *    derived from this software without prior written permission. For<br/>
 *    written permission, please contact apache@apache.org.<br/>
 *<br/>
 * 5. Products derived from this software may not be called "Apache",<br/>
 *    "Apache Lucene", nor may "Apache" appear in their name, without<br/>
 *    prior written permission of the Apache Software Foundation.<br/>
 *<br/>
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED<br/>
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES<br/>
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE<br/>
 * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR<br/>
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,<br/>
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT<br/>
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF<br/>
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND<br/>
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,<br/>
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT<br/>
 * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF<br/>
 * SUCH DAMAGE.<br/>
 * ====================================================================<br/>
 *<br/>
 * This software consists of voluntary contributions made by many<br/>
 * individuals on behalf of the Apache Software Foundation.  For more<br/>
 * information on the Apache Software Foundation, please see<br/>
 * &lt;http://www.apache.org/&gt;.<br/>
 */<br/>
<br/>
import java.io.IOException;<br/>
import java.io.File;<br/>
import java.io.PrintStream;<br/>
import java.util.Vector;<br/>
<br/>
import org.apache.lucene.store.Directory;<br/>
import org.apache.lucene.store.RAMDirectory;<br/>
import org.apache.lucene.store.FSDirectory;<br/>
import org.apache.lucene.store.Lock;<br/>
import org.apache.lucene.store.InputStream;<br/>
import org.apache.lucene.store.OutputStream;<br/>
import org.apache.lucene.document.Document;<br/>
import org.apache.lucene.analysis.Analyzer;<br/>
<br/>
/**<br/>
  An IndexWriter creates and maintains an index.<br/>
<br/>
  The third argument to the &lt;a href="#IndexWriter"&gt;&lt;b&gt;constructor&lt;/b&gt;&lt;/a&gt;<br/>
  determines whether a new index is created, or whether an existing index is<br/>
  opened for the addition of new documents.<br/>
<br/>
  In either case, documents are added with the &lt;a<br/>
  href="#addDocument"&gt;&lt;b&gt;addDocument&lt;/b&gt;&lt;/a&gt; method.  When finished adding<br/>
  documents, &lt;a href="#close"&gt;&lt;b&gt;close&lt;/b&gt;&lt;/a&gt; should be called.<br/>
<br/>
  If an index will not have more documents added for a while and optimal search<br/>
  performance is desired, then the &lt;a href="#optimize"&gt;&lt;b&gt;optimize&lt;/b&gt;&lt;/a&gt;<br/>
  method should be called before the index is closed.<br/>
  */<br/>
<br/>
public final class IndexWriter {<br/>
  private Directory directory;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // where this index resides<br/>
  private Analyzer analyzer;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // how to analyze text<br/>
<br/>
  private SegmentInfos segmentInfos = new SegmentInfos(); // the segments<br/>
  private final Directory ramDirectory = new RAMDirectory(); // for temp segs<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;path&lt;/code&gt;.  Text will<br/>
    be analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a<br/>
    new, empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index<br/>
    already there, if any. */<br/>
  public IndexWriter(String path, Analyzer a, boolean create)<br/>
       throws IOException {<br/>
    this(FSDirectory.getDirectory(path, create), a, create);<br/>
  }<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;path&lt;/code&gt;.  Text will<br/>
    be analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a<br/>
    new, empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index<br/>
    already there, if any. */<br/>
  public IndexWriter(File path, Analyzer a, boolean create)<br/>
       throws IOException {<br/>
    this(FSDirectory.getDirectory(path, create), a, create);<br/>
  }<br/>
<br/>
  /** Constructs an IndexWriter for the index in &lt;code&gt;d&lt;/code&gt;.  Text will be<br/>
    analyzed with &lt;code&gt;a&lt;/code&gt;.  If &lt;code&gt;create&lt;/code&gt; is true, then a new,<br/>
    empty index will be created in &lt;code&gt;d&lt;/code&gt;, replacing the index already<br/>
    there, if any. */<br/>
  public IndexWriter(Directory d, Analyzer a, final boolean create)<br/>
       throws IOException {<br/>
    directory = d;<br/>
    analyzer = a;<br/>
<br/>
    Lock writeLock = directory.makeLock("write.lock");<br/>
    if (!writeLock.obtain())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // obtain write lock<br/>
      throw new IOException("Index locked for write: " + writeLock);<br/>
<br/>
    synchronized (directory) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // in- &amp; inter-process sync<br/>
      new Lock.With(directory.makeLock("commit.lock")) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  public Object doBody() throws IOException {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    if (create)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;      segmentInfos.write(directory);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;      segmentInfos.read(directory);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    return null;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  }<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}.run();<br/>
    }<br/>
  }<br/>
<br/>
  /** Flushes all changes to an index, closes all associated files, and closes<br/>
    the directory that the index is stored in. */<br/>
  public final synchronized void close() throws IOException {<br/>
    flushRamSegments();<br/>
    ramDirectory.close();<br/>
    directory.makeLock("write.lock").release();  // release write lock<br/>
    directory.close();<br/>
  }<br/>
<br/>
  /** Returns the number of documents currently in this index. */<br/>
  public final synchronized int docCount() {<br/>
    int count = 0;<br/>
    for (int i = 0; i &lt; segmentInfos.size(); i++) {<br/>
      SegmentInfo si = segmentInfos.info(i);<br/>
      count += si.docCount;<br/>
    }<br/>
    return count;<br/>
  }<br/>
<br/>
  /** The maximum number of terms that will be indexed for a single field in a<br/>
    document.  This limits the amount of memory required for indexing, so that<br/>
    collections with very large files will not crash the indexing process by<br/>
    running out of memory.<br/>
<br/>
    &lt;p&gt;By default, no more than 10,000 terms will be indexed for a field. */<br/>
  public int maxFieldLength = 10000;<br/>
<br/>
  /** Adds a document to this index.*/<br/>
  public final void addDocument(Document doc) throws IOException {<br/>
    DocumentWriter dw =<br/>
      new DocumentWriter(ramDirectory, analyzer, maxFieldLength);<br/>
    String segmentName = newSegmentName();<br/>
    dw.addDocument(segmentName, doc);<br/>
    synchronized (this) {<br/>
      segmentInfos.addElement(new SegmentInfo(segmentName, 1, ramDirectory));<br/>
      maybeMergeSegments();<br/>
    }<br/>
  }<br/>
<br/>
  private final synchronized String newSegmentName() {<br/>
    return "_" + Integer.toString(segmentInfos.counter++, Character.MAX_RADIX);<br/>
  }<br/>
<br/>
  /** Determines how often segment indexes are merged by addDocument().  With<br/>
   * smaller values, less RAM is used while indexing, and searches on<br/>
   * unoptimized indexes are faster, but indexing speed is slower.  With larger<br/>
   * values more RAM is used while indexing and searches on unoptimized indexes<br/>
   * are slower, but indexing is faster.  Thus larger values (&gt; 10) are best<br/>
   * for batched index creation, and smaller values (&lt; 10) for indexes that are<br/>
   * interactively maintained.<br/>
   *<br/>
   * &lt;p&gt;This must never be less than 2.  The default value is 10.*/<br/>
  public int mergeFactor = 10;<br/>
<br/>
  /** Determines the largest number of documents ever merged by addDocument().<br/>
   * Small values (e.g., less than 10,000) are best for interactive indexing,<br/>
   * as this limits the length of pauses while indexing to a few seconds.<br/>
   * Larger values are best for batched indexing and speedier searches.<br/>
   *<br/>
   * &lt;p&gt;The default value is {@link Integer#MAX_VALUE}. */<br/>
  public int maxMergeDocs = Integer.MAX_VALUE;<br/>
<br/>
  /** If non-null, information about merges will be printed to this. */<br/>
  public PrintStream infoStream = null;<br/>
<br/>
  /** Merges all segments together into a single segment, optimizing an index<br/>
      for search. */<br/>
  public final synchronized void optimize() throws IOException {<br/>
    flushRamSegments();<br/>
    while (<span class="add"><span class="mv">segmentInfos.size() &gt; 1</span> ||<br/>
&nbsp;&nbsp;&nbsp;&nbsp;   <span class="add">(<span class="mv"><span class="upd"><span class="mv">segmentInfos.size() == 1</span> &amp;&amp;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    (<span class="upd">SegmentReader.hasDeletions(segmentInfos.info(0)) ||<br/>
             <span class="add"><span class="add"><span class="add"><span class="add">segmentInfos</span>.<span class="add">info</span>(<span class="add">0</span>)</span>.<span class="add">dir</span></span> != <span class="add">directory</span></span></span>)</span></span>)</span></span>) {<br/>
      int minSegment = segmentInfos.size() - mergeFactor;<br/>
      mergeSegments(minSegment &lt; 0 ? 0 : minSegment);<br/>
    }<br/>
  }<br/>
<br/>
  /** Merges all segments from an array of indexes into this index.<br/>
   *<br/>
   * &lt;p&gt;This may be used to parallelize batch indexing.  A large document<br/>
   * collection can be broken into sub-collections.  Each sub-collection can be<br/>
   * indexed in parallel, on a different thread, process or machine.  The<br/>
   * complete index can then be created by merging sub-collection indexes<br/>
   * with this method.<br/>
   *<br/>
   * &lt;p&gt;After this completes, the index is optimized. */<br/>
  public final synchronized void addIndexes(Directory[] dirs)<br/>
      throws IOException {<br/>
    optimize();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // start with zero or 1 seg<br/>
    int minSegment = segmentInfos.size();<br/>
    int segmentsAddedSinceMerge = 0;<br/>
    for (int i = 0; i &lt; dirs.length; i++) {<br/>
      SegmentInfos sis = new SegmentInfos();&nbsp;&nbsp;&nbsp;&nbsp;  // read infos from dir<br/>
      sis.read(dirs[i]);<br/>
      for (int j = 0; j &lt; sis.size(); j++) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;segmentInfos.addElement(sis.info(j));&nbsp;&nbsp;&nbsp;&nbsp;  // add each info<br/>
<br/>
&nbsp;&nbsp;&nbsp;&nbsp;// merge whenever mergeFactor segments have been added<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (++segmentsAddedSinceMerge == mergeFactor) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  mergeSegments(minSegment++, false);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  segmentsAddedSinceMerge = 0;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
      }<br/>
    }<br/>
    optimize();&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // final cleanup<br/>
  }<br/>
<br/>
  /** Merges all RAM-resident segments. */<br/>
  private final void flushRamSegments() throws IOException {<br/>
    int minSegment = segmentInfos.size()-1;<br/>
    int docCount = 0;<br/>
    while (minSegment &gt;= 0 &amp;&amp;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;   (segmentInfos.info(minSegment)).dir == ramDirectory) {<br/>
      docCount += segmentInfos.info(minSegment).docCount;<br/>
      minSegment--;<br/>
    }<br/>
    if (minSegment &lt; 0 ||&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // add one FS segment?<br/>
&nbsp;&nbsp;&nbsp;&nbsp;(docCount + segmentInfos.info(minSegment).docCount) &gt; mergeFactor ||<br/>
&nbsp;&nbsp;&nbsp;&nbsp;!(segmentInfos.info(segmentInfos.size()-1).dir == ramDirectory))<br/>
      minSegment++;<br/>
    if (minSegment &gt;= segmentInfos.size())<br/>
      return;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // none to merge<br/>
    mergeSegments(minSegment);<br/>
  }<br/>
<br/>
  /** Incremental segment merger.  */<br/>
  private final void maybeMergeSegments() throws IOException {<br/>
    long targetMergeDocs = mergeFactor;<br/>
    while (targetMergeDocs &lt;= maxMergeDocs) {<br/>
      // find segments smaller than current target size<br/>
      int minSegment = segmentInfos.size();<br/>
      int mergeDocs = 0;<br/>
      while (--minSegment &gt;= 0) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;SegmentInfo si = segmentInfos.info(minSegment);<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (si.docCount &gt;= targetMergeDocs)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  break;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;mergeDocs += si.docCount;<br/>
      }<br/>
<br/>
      if (mergeDocs &gt;= targetMergeDocs)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // found a merge to do<br/>
&nbsp;&nbsp;&nbsp;&nbsp;mergeSegments(minSegment+1);<br/>
      else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;break;<br/>
      <br/>
      targetMergeDocs *= mergeFactor;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // increase target size<br/>
    }<br/>
  }<br/>
<br/>
  /** Pops segments off of segmentInfos stack down to minSegment, merges them,<br/>
    and pushes the merged index onto the top of the segmentInfos stack. */<br/>
  private final void mergeSegments(int minSegment) throws IOException {<br/>
    mergeSegments(minSegment, true);<br/>
  }<br/>
<br/>
  /** Pops segments off of segmentInfos stack down to minSegment, merges them,<br/>
    and pushes the merged index onto the top of the segmentInfos stack. */<br/>
  private final void mergeSegments(int minSegment, boolean delete)<br/>
      throws IOException {<br/>
    String mergedName = newSegmentName();<br/>
    int mergedDocCount = 0;<br/>
    if (infoStream != null) infoStream.print("merging segments");<br/>
    SegmentMerger merger = new SegmentMerger(directory, mergedName);<br/>
    final Vector segmentsToDelete = new Vector();<br/>
    for (int i = minSegment; i &lt; segmentInfos.size(); i++) {<br/>
      SegmentInfo si = segmentInfos.info(i);<br/>
      if (infoStream != null)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;infoStream.print(" " + si.name + " (" + si.docCount + " docs)");<br/>
      SegmentReader reader = new SegmentReader(si);<br/>
      merger.add(reader);<br/>
      if (delete)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;segmentsToDelete.addElement(reader);&nbsp;&nbsp;&nbsp;&nbsp;  // queue for deletion<br/>
      mergedDocCount += si.docCount;<br/>
    }<br/>
    if (infoStream != null) {<br/>
      infoStream.println();<br/>
      infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");<br/>
    }<br/>
    merger.merge();<br/>
<br/>
    segmentInfos.setSize(minSegment);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // pop old infos &amp; add new<br/>
    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    directory));<br/>
    <br/>
    synchronized (directory) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // in- &amp; inter-process sync<br/>
      new Lock.With(directory.makeLock("commit.lock")) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  public Object doBody() throws IOException {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    segmentInfos.write(directory);&nbsp;&nbsp;&nbsp;&nbsp;  // commit before deleting<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    deleteSegments(segmentsToDelete);&nbsp;&nbsp;&nbsp;&nbsp;  // delete now-unused segments<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    return null;<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  }<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}.run();<br/>
    }<br/>
  }<br/>
<br/>
  /* Some operating systems (e.g. Windows) don't permit a file to be deleted<br/>
     while it is opened for read (e.g. by another process or thread).  So we<br/>
     assume that when a delete fails it is because the file is open in another<br/>
     process, and queue the file for subsequent deletion. */<br/>
<br/>
  private final void deleteSegments(Vector segments) throws IOException {<br/>
    Vector deletable = new Vector();<br/>
<br/>
    deleteFiles(readDeleteableFiles(), deletable); // try to delete deleteable<br/>
    <br/>
    for (int i = 0; i &lt; segments.size(); i++) {<br/>
      SegmentReader reader = (SegmentReader)segments.elementAt(i);<br/>
      if (reader.directory == this.directory)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;deleteFiles(reader.files(), deletable);&nbsp;&nbsp;&nbsp;&nbsp;  // try to delete our files<br/>
      else<br/>
&nbsp;&nbsp;&nbsp;&nbsp;deleteFiles(reader.files(), reader.directory); // delete, eg, RAM files<br/>
    }<br/>
<br/>
    writeDeleteableFiles(deletable);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // note files we can't delete<br/>
  }<br/>
<br/>
  private final void deleteFiles(Vector files, Directory directory)<br/>
       throws IOException {<br/>
    for (int i = 0; i &lt; files.size(); i++)<br/>
      directory.deleteFile((String)files.elementAt(i));<br/>
  }<br/>
<br/>
  private final void deleteFiles(Vector files, Vector deletable)<br/>
       throws IOException {<br/>
    for (int i = 0; i &lt; files.size(); i++) {<br/>
      String file = (String)files.elementAt(i);<br/>
      try {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;directory.deleteFile(file);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // try to delete each file<br/>
      } catch (IOException e) {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // if delete fails<br/>
&nbsp;&nbsp;&nbsp;&nbsp;if (directory.fileExists(file)) {<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  if (infoStream != null)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;    infoStream.println(e.getMessage() + "; Will re-try later.");<br/>
&nbsp;&nbsp;&nbsp;&nbsp;  deletable.addElement(file);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  // add to deletable<br/>
&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
      }<br/>
    }<br/>
  }<br/>
<br/>
  private final Vector readDeleteableFiles() throws IOException {<br/>
    Vector result = new Vector();<br/>
    if (!directory.fileExists("deletable"))<br/>
      return result;<br/>
<br/>
    InputStream input = directory.openFile("deletable");<br/>
    try {<br/>
      for (int i = input.readInt(); i &gt; 0; i--)&nbsp;&nbsp;&nbsp;&nbsp;  // read file names<br/>
&nbsp;&nbsp;&nbsp;&nbsp;result.addElement(input.readString());<br/>
    } finally {<br/>
      input.close();<br/>
    }<br/>
    return result;<br/>
  }<br/>
<br/>
  private final void writeDeleteableFiles(Vector files) throws IOException {<br/>
    OutputStream output = directory.createFile("deleteable.new");<br/>
    try {<br/>
      output.writeInt(files.size());<br/>
      for (int i = 0; i &lt; files.size(); i++)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;output.writeString((String)files.elementAt(i));<br/>
    } finally {<br/>
      output.close();<br/>
    }<br/>
    directory.renameFile("deleteable.new", "deletable");<br/>
  }<br/>
}<br/>
</div>
</div>
</div>
<div class="clear"></div>
</div>
</body>
</html>